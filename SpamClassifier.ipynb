{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text class\n",
       "0  Go until jurong point, crazy.. Available only ...   ham\n",
       "1                      Ok lar... Joking wif u oni...   ham\n",
       "2  Free entry in 2 a wkly comp to win FA Cup fina...  spam\n",
       "3  U dun say so early hor... U c already then say...   ham\n",
       "4  Nah I don't think he goes to usf, he lives aro...   ham"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load dataset and reorder column\n",
    "df = pd.read_table('sms', names=['class', 'text'])\n",
    "df = df[['text', 'class']]\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"stopwords.txt\", \"r\")\n",
    "stopwords = f.read().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stopword_removal(df, stopwords):\n",
    "    row, col = df.shape\n",
    "    for i in range(row):\n",
    "        sentence = df.loc[i]['text']\n",
    "        cleaned = []\n",
    "        for word in sentence.split():\n",
    "            if word not in stopwords:\n",
    "                cleaned.append(word)\n",
    "        df.loc[i]['text'] = ' '.join(cleaned)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize(df):\n",
    "    row, col = df.shape\n",
    "    for i in range(row):\n",
    "        sentence = df.loc[i]['text']\n",
    "        lowercase = sentence.lower() # case folding\n",
    "        regex = re.compile(r'[^a-zA-Z]')\n",
    "        alphanumeric = re.sub(regex, ' ', lowercase) # remove if not alphabet\n",
    "        stripped = ' '.join(alphanumeric.split()) # strip whitespace\n",
    "        df.loc[i]['text'] = stripped\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stem(df):\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    row, col = df.shape\n",
    "    for i in range(row):\n",
    "        sentence = df.loc[i]['text']\n",
    "        stemed_words = []\n",
    "        for word in sentence.split():\n",
    "            stemed_word = stemmer.stem(word)\n",
    "            stemed_words.append(stemed_word)\n",
    "        df.loc[i]['text'] = ' '.join(stemed_words)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = tokenize(df)\n",
    "filtered = stopword_removal(tokenized, stopwords)\n",
    "stemed = stem(filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_unique_words(df):\n",
    "    row, col = df.shape\n",
    "    unique_words = set()\n",
    "    for i in range(row):\n",
    "        sentence = df.loc[i]['text']\n",
    "        for word in sentence.split():\n",
    "            unique_words.add(word)\n",
    "    return unique_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_tf_matrix(df, unique_words):\n",
    "    tf_matrix = []\n",
    "    row, col = df.shape\n",
    "    word_freq = {}\n",
    "\n",
    "    #set frequency to 0 for all term\n",
    "    for word in unique_words:\n",
    "        word_freq[word] = 0\n",
    "\n",
    "    for i in range(row):\n",
    "        sentence = df.loc[i]['text']\n",
    "        freq = word_freq.copy()\n",
    "        #increment every word frequency in sentence\n",
    "        for word in sentence.split():\n",
    "            freq[word] += 1\n",
    "\n",
    "        tf_matrix.append(freq)\n",
    "    return tf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tf_matrix_to_array(tf_matrix):\n",
    "    tf_matrix_array = []\n",
    "    row = len(tf_matrix)\n",
    "    for i in range(row):\n",
    "        feature = np.array(list(tf_matrix[i].values()))\n",
    "        tf_matrix_array.append(feature)\n",
    "    return np.vstack(tf_matrix_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unique_words = get_unique_words(df)\n",
    "tf_matrix = get_tf_matrix(df, unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tf_matrix_to_array(tf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#split training and training data set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, df['class'], test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create and train mnb classifier\n",
    "classifier = MultinomialNB() # buat classifier\n",
    "classifier.fit(X_train, y_train)  # training classifier dengan data training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['spam'], \n",
       "      dtype='<U4')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = pd.DataFrame({'text': [\n",
    "       \"Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\"\n",
    "]})\n",
    "tokenized = tokenize(sample)\n",
    "filtered = stopword_removal(tokenized, stopwords)\n",
    "stemed = stem(filtered)\n",
    "tf_matrix = get_tf_matrix(stemed, unique_words)\n",
    "tf_array = tf_matrix_to_array(tf_matrix)\n",
    "classifier.predict(tf_array) # ubah input term frequency ke array dan lakukan prediksi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive bayes accuracy : 97.130045%\n"
     ]
    }
   ],
   "source": [
    "# measure accuracy\n",
    "n, feature_size = X_test.shape\n",
    "accuracy = np.sum(classifier.predict(X_test) == y_test) / n # sum of correct prediction / number of test dataset\n",
    "print(\"Naive bayes accuracy : %f%%\" % (accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "164\n",
      "951\n"
     ]
    }
   ],
   "source": [
    "print(np.sum(classifier.predict(X_test) == 'spam'))\n",
    "print(np.sum(classifier.predict(X_test) == 'ham'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
